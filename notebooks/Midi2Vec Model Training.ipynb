{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gensim\n",
    "import gensim.models.doc2vec\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "import re\n",
    "import time\n",
    "import itertools\n",
    "import multiprocessing\n",
    "\n",
    "from random import shuffle\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 documents\n",
      "Loaded 200000 documents\n",
      "Loaded 300000 documents\n",
      "Loaded 400000 documents\n",
      "Loaded 500000 documents\n",
      "Loaded 600000 documents\n",
      "Loaded 700000 documents\n",
      "Loaded 800000 documents\n",
      "Loaded 900000 documents\n",
      "Loaded 1000000 documents\n",
      "Loaded 1100000 documents\n",
      "Loaded 1200000 documents\n",
      "Loaded 1300000 documents\n",
      "Loaded 1400000 documents\n",
      "Loaded 1500000 documents\n",
      "Loaded 1600000 documents\n",
      "Loaded 1700000 documents\n"
     ]
    }
   ],
   "source": [
    "TrainingDocument = namedtuple('TrainingDocument', 'words tags')\n",
    "\n",
    "docs = []\n",
    "processed_doc_count = 0\n",
    "with open(\"../../midi-embeddings/data/1_measure_binned_full.txt\", 'rb') as data:\n",
    "    for line_no, line in enumerate(data, 1):\n",
    "        tokens = [x.strip() for x in gensim.utils.to_unicode(line).split(\",\")]\n",
    "        words = tokens[0:]\n",
    "        tags = [line_no]\n",
    "        docs.append(TrainingDocument(words, tags))\n",
    "        processed_doc_count += 1\n",
    "        if processed_doc_count % 100000 == 0:\n",
    "            print(\"Loaded \" + str(processed_doc_count) + \" documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(params):\n",
    "    param_vals = [(shorten_param_name(key) + \"_\" + str(value)) for key, value in params.items()]\n",
    "    return '-'.join(sorted(param_vals))\n",
    "\n",
    "def shorten_param_name(name):\n",
    "    name = re.sub(\"doc2vec_\", \"\", name)\n",
    "    name = re.sub(\"_\", \"\", name)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params):\n",
    "    \"\"\"\n",
    "    Trains the document vector model as configured.\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "    # Model parameters:\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    dm = params['doc2vec_dm']\n",
    "    dm_mean = params['doc2vec_dm_mean']\n",
    "    end_alpha = params['doc2vec_learning_rate_end']\n",
    "    epochs = params['doc2vec_epochs']\n",
    "    hs = params['doc2vec_hs']\n",
    "    negative = params['doc2vec_negative']\n",
    "    min_count = params['doc2vec_min_count']\n",
    "    start_alpha = params['doc2vec_learning_rate_start']\n",
    "    vector_size = params['doc2vec_vector_size']\n",
    "    window = params['doc2vec_window']\n",
    "\n",
    "    print(\"Training encoder model...\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Create model\n",
    "    model = Doc2Vec(dm=dm,\n",
    "                    dm_mean=dm_mean,\n",
    "                    vector_size=vector_size,\n",
    "                    window=window,\n",
    "                    negative=negative,\n",
    "                    hs=hs,\n",
    "                    min_count=min_count,\n",
    "                    workers=cores)\n",
    "\n",
    "    model.build_vocab(docs)\n",
    "\n",
    "    # Train model\n",
    "    model.train(docs,\n",
    "                total_examples=len(docs),\n",
    "                epochs=epochs,\n",
    "                start_alpha=start_alpha,\n",
    "                end_alpha=end_alpha)\n",
    "\n",
    "    end = time.time()\n",
    "    message = \"Trained encoder model in \" + str(end - start) + \" seconds\"\n",
    "    print(message)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTE_NAMES = [\"c\", \"c#\", \"d\", \"d#\", \"e\", \"f\", \"f#\", \"g\", \"g#\", \"a\", \"a#\", \"b\"]\n",
    "\n",
    "def _get_instrument(word):\n",
    "    fields = word.split(\"_\")\n",
    "    if len(fields) > 1:\n",
    "        instrument = fields[0]\n",
    "        return instrument\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def _get_note_and_octave(word):\n",
    "    fields = word.split(\"_\")\n",
    "    if len(fields) > 1:\n",
    "        note_field = fields[1]\n",
    "        octave = _get_octave(note_field)\n",
    "        note = ''.join([i for i in note_field if not i.isdigit()])\n",
    "        return (note, octave)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def _get_duration(word):\n",
    "    fields = word.split(\"_\")\n",
    "    if len(fields) > 1:\n",
    "        return fields[2]\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "def _get_octave(s):\n",
    "    m = re.search(r'\\d+$', s)\n",
    "    return int(m.group()) if m else None\n",
    "\n",
    "def _get_fifth(note, octave):\n",
    "    note_index = NOTE_NAMES.index(note)\n",
    "    fifth_index = (note_index + 7)\n",
    "    if fifth_index > len(NOTE_NAMES) - 1:\n",
    "        fifth_index = fifth_index % 12\n",
    "        octave +=1\n",
    "    return (NOTE_NAMES[fifth_index], octave)\n",
    "\n",
    "def _get_fifth_word(word):\n",
    "    instrument = _get_instrument(word)\n",
    "    duration = _get_duration(word)\n",
    "    note_and_octave = _get_note_and_octave(word)\n",
    "    if instrument is not None and duration is not None and note_and_octave is not None:\n",
    "        (note, octave) = note_and_octave\n",
    "        (note, octave) = _get_fifth(note, octave)\n",
    "        word = instrument + \"_\" + note + str(octave) + \"_\" + str(duration)\n",
    "        return word\n",
    "    return None\n",
    "\n",
    "def get_fifth_score(model):\n",
    "    scores = []\n",
    "    for word in model.wv.vocab:\n",
    "        fields = word.split(\"_\")\n",
    "        if len(fields) > 1 and fields[0] != \"percussion\":\n",
    "            fifth = _get_fifth_word(word)\n",
    "            if fifth in model.wv.vocab:\n",
    "                scores.append(model.wv.similarity(word, fifth))\n",
    "    return sum(scores) / float(len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 36 combinations\n",
      "dm_1-dmmean_1-epochs_8-hs_0-learningrateend_0.2-learningratestart_0.025-mincount_32-negative_128-vectorsize_20-window_1\n",
      "Training encoder model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-874916f11135>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fifth_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mcombination\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"score\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-63002db7c64b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 end_alpha=end_alpha)\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, documents, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_raw_word_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m             trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    256\u001b[0m                 \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                 queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    258\u001b[0m             \u001b[0mtrained_word_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrained_word_count_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mraw_word_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mraw_word_count_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    225\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    226\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             report_delay=report_delay)\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'doc2vec_dm': [1],\n",
    "    'doc2vec_dm_mean': [1],\n",
    "    'doc2vec_epochs': [2,4,8,16],\n",
    "    'doc2vec_hs': [0],\n",
    "    'doc2vec_learning_rate_start': [0.025],\n",
    "    'doc2vec_learning_rate_end': [0.2],\n",
    "    'doc2vec_min_count': [16,24,32],\n",
    "    'doc2vec_negative': [32,64,128],\n",
    "    'doc2vec_vector_size': [20],\n",
    "    'doc2vec_window': [1]\n",
    "}\n",
    "\n",
    "values = [[(key, value) for value in values] for (key, values) in sorted(params.items())]\n",
    "combinations = list(itertools.product(*values))\n",
    "print(\"Defined {} combinations\".format(len(combinations)))\n",
    "\n",
    "shuffle(combinations)\n",
    "\n",
    "for combination in combinations:\n",
    "    combination = dict(combination)\n",
    "    model_id = get_id(combination)\n",
    "    print(model_id)\n",
    "    model = train(combination)\n",
    "    score = get_fifth_score(model)\n",
    "    combination[\"score\"] = score\n",
    "    print(\"- score: {}\".format(score))\n",
    "    eval_result_rows.append(combination)\n",
    "    model.save(\"doc2vec/\" + model_id)\n",
    "    del model\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(eval_result_rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = \"doc2vec_negative\"\n",
    "\n",
    "grouped = df[[variable,'score']].groupby([variable], as_index=False).median()\n",
    "sns.pointplot(x=variable, y='score', data=grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = \"doc2vec_epochs\"\n",
    "\n",
    "grouped = df[[variable,'score']].groupby([variable], as_index=False).median()\n",
    "sns.pointplot(x=variable, y='score', data=grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = \"doc2vec_min_count\"\n",
    "\n",
    "grouped = df[[variable,'score']].groupby([variable], as_index=False).median()\n",
    "sns.pointplot(x=variable, y='score', data=grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "heatmap_x = \"doc2vec_negative\"\n",
    "heatmap_y = \"doc2vec_min_count\"\n",
    "\n",
    "#filtered = df[df['doc2vec_window'].astype(int) == 1]\n",
    "grouped = df.groupby([heatmap_y, heatmap_x], as_index=False).median()\n",
    "pivoted = grouped.pivot(heatmap_y, heatmap_x, \"score\")\n",
    "sns.heatmap(pivoted, annot=False, fmt=\"g\", cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
