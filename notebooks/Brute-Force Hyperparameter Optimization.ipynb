{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Add Midi2Vec to Python working directory\n",
    "sys.path.append('../')\n",
    "\n",
    "from data_loading.data_loaders import MidiDataLoader\n",
    "from midi_to_dataframe.note_mapper import NoteMapper\n",
    "from pipeline.pipeline import Pipeline\n",
    "from optimization.optimizers import BruteForce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.level = logging.INFO\n",
    "stream_handler = logging.StreamHandler(sys.stdout)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Documents used to train semantic encoder model\n",
    "encoder_training_docs = \"../../midi-embeddings/data/full_1_measure.txt\"\n",
    "\n",
    "param_sweep_values = {\n",
    "\n",
    "    # Encoder (doc2vec) settings:\n",
    "    'doc2vec_docs': [encoder_training_docs],\n",
    "    'doc2vec_dm': [1],\n",
    "    'doc2vec_dm_mean': [1],\n",
    "    'doc2vec_epochs': [1,2],\n",
    "    'doc2vec_hs': [0],\n",
    "    'doc2vec_learning_rate_start': [0.025],\n",
    "    'doc2vec_learning_rate_end': [0.2],\n",
    "    'doc2vec_min_count': [5,10],\n",
    "    'doc2vec_negative': [0],\n",
    "    'doc2vec_vector_size': [20,24],\n",
    "    'doc2vec_window': [1,2,3],\n",
    "\n",
    "    # Sequence learning (Keras LSTM) settings:\n",
    "    'nn_features': [['bpm', 'measure', 'beat']],\n",
    "    'nn_batch_size': [25,50,75],\n",
    "    'nn_dense_activation_function': [\"linear\"],\n",
    "    'nn_dropout': [0],\n",
    "    'nn_epochs': [20,30,40,50],\n",
    "    'nn_hidden_neurons': [15,20,30],\n",
    "    'nn_layers': [15,20,25],\n",
    "    'nn_lstm_activation_function': [\"selu\"],\n",
    "    'nn_lstm_n_prev': [12,16,20,24]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define note mapper for MIDI file loading\n",
    "note_mapping_config_path = \"../settings/map-to-group.json\"\n",
    "note_mapper = NoteMapper(note_mapping_config_path)\n",
    "\n",
    "# Data loader used to encode MIDI-format training files\n",
    "data_loader = MidiDataLoader(note_mapper)\n",
    "\n",
    "# Define training documents for sequence learning\n",
    "training_docs = [\"../resources/breakbeats/084 Breakthru.mid\", \"../resources/breakbeats/086 Clouds.mid\",\n",
    "                 \"../resources/breakbeats/089 Get Out.mid\", \"../resources/breakbeats/089 Wrong.mid\",\n",
    "                 \"../resources/breakbeats/090 Deceive.mid\", \"../resources/breakbeats/090 New York.mid\",\n",
    "                 \"../resources/breakbeats/090 Radio.mid\", \"../resources/breakbeats/093 Pretender.mid\",\n",
    "                 \"../resources/breakbeats/093 Right Won.mid\", \"../resources/breakbeats/094 Run.mid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_dfs = []\n",
    "runs_completed = [0]\n",
    "max_runs = 2\n",
    "\n",
    "def callback(params, metrics):\n",
    "    runs_completed[0] += 1\n",
    "    merged = {**params, **metrics}\n",
    "    merged['nn_features'] = ', '.join(merged['nn_features'])\n",
    "    print(\"Completed \" + str(runs_completed) + \" runs.\")\n",
    "    print(merged)\n",
    "    results_dfs.append(pd.DataFrame(merged, index=[0]))\n",
    "    if runs_completed[0] >= max_runs:\n",
    "        print(\"Max. runcount reached.\")\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BruteForce evaluation of 10368 hyperparameter combinations.\n",
      "Loaded 100000 documents\n",
      "Loaded 200000 documents\n",
      "Loaded 300000 documents\n",
      "Loaded 400000 documents\n",
      "Loaded 500000 documents\n",
      "Loaded 600000 documents\n",
      "Loaded 700000 documents\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline()\n",
    "pipeline.set_data_loader(data_loader)\n",
    "pipeline.set_training_docs(training_docs)\n",
    "pipeline.set_k_fold_cross_eval(k=3)\n",
    "\n",
    "brute_force_param_sweep = BruteForce(params=param_sweep_values)\n",
    "brute_force_param_sweep.set_callback(callback)\n",
    "pipeline.set_optimizer(brute_force_param_sweep)\n",
    "\n",
    "results_df = pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#results_df = pd.concat(results_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "variables = []\n",
    "for key, value in param_sweep_values.items():\n",
    "    if len(value) > 1:\n",
    "        variables.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for variable in variables:\n",
    "    print(results_df[[variable,'f1']].groupby([variable], as_index=False).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# TODO loops not necessary...\n",
    "num_plots = 0\n",
    "for heatmap_x in variables:\n",
    "    for heatmap_y in variables:\n",
    "        if heatmap_x != heatmap_y:\n",
    "            num_plots += 1\n",
    "            \n",
    "dim = math.ceil(num_plots / 3)\n",
    "\n",
    "fig, axes = plt.subplots(dim, 3, sharex=False, sharey=False)\n",
    "fig.set_size_inches(20, 12)\n",
    "\n",
    "axis_index = 0\n",
    "for heatmap_x in variables:\n",
    "    for heatmap_y in variables:\n",
    "        if heatmap_x != heatmap_y:\n",
    "            pivoted = results_df.groupby([heatmap_y, heatmap_x], as_index=False).median().pivot(heatmap_y, heatmap_x, \"f1\")\n",
    "            sns.heatmap(pivoted, annot=True, fmt=\"g\", cmap='viridis', ax=axes.flat[axis_index])\n",
    "            axis_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
