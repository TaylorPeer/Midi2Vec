{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brute-Force Hyperparameter Optimization\n",
    "\n",
    "This notebook shows how to use the *BruteForce* optimizer module together with a **Midi2Vec** *Pipeline* to evaluate a large number of *Encoder* and *SequenceLearner* hyperparameter combinations against a given training set.\n",
    "\n",
    "* **Encoder**: trains a *Doc2Vec* model against textual representations of MIDI files and uses this model to convert MIDI files into sequences of real-valued vectors.\n",
    "* **SequenceLearner**: uses a *Keras* LSTM to learn patterns in sequences of real-valued vectors and, once trained, can generate new sequences of such vectors.\n",
    "* **Pipeline**: combines various **Midi2Vec** components for encoding, sequence learning and evaluation into a single interface.\n",
    "* **BruteForce**: Computes and evaluates all possible hyperparameter combinations within user-specified ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "# Add Midi2Vec to Python working directory\n",
    "sys.path.append('../')\n",
    "\n",
    "from data_loading import MidiDataLoader\n",
    "from midi_to_dataframe import NoteMapper\n",
    "from pipeline import Pipeline\n",
    "from optimization import BruteForce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.level = logging.INFO\n",
    "stream_handler = logging.StreamHandler(sys.stdout)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "# Disable Gensim (Doc2Vec) output\n",
    "logging.getLogger(\"gensim\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute-Force Hyperparameter Sweep Values\n",
    "\n",
    "All individual hyperparameter values to evaluate should be explicitly defined below. The *BruteForce* module will then compute all of their possible permutations and train and evaluate the corresponding modules using each computed set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Documents used to train semantic encoder model\n",
    "encoder_training_docs = \"../../midi-embeddings/data/full_1_measure.txt\"\n",
    "#encoder_training_docs = \"../../midi-embeddings/data/full_1_measure_100k.txt\"\n",
    "\n",
    "param_sweep_values = {\n",
    "\n",
    "    # Encoder (doc2vec) settings:\n",
    "    'doc2vec_docs': [encoder_training_docs],\n",
    "    'doc2vec_dm': [1],\n",
    "    'doc2vec_dm_mean': [1],\n",
    "    'doc2vec_epochs': [2],\n",
    "    'doc2vec_hs': [0],\n",
    "    'doc2vec_learning_rate_start': [0.025],\n",
    "    'doc2vec_learning_rate_end': [0.2],\n",
    "    'doc2vec_min_count': [10],\n",
    "    'doc2vec_negative': [0],\n",
    "    'doc2vec_vector_size': [20],\n",
    "    'doc2vec_window': [5],\n",
    "\n",
    "    # Sequence learning (Keras LSTM) settings:\n",
    "    'nn_features': [['bpm', 'measure', 'beat']],\n",
    "    'nn_batch_size': [15],\n",
    "    'nn_dense_activation_function': [\"linear\"],\n",
    "    'nn_dropout': [0.05],\n",
    "    'nn_epochs': [40],\n",
    "    'nn_hidden_neurons': [40],\n",
    "    'nn_layers': [15, 30],\n",
    "    'nn_lstm_activation_function': [\"selu\"],\n",
    "    'nn_lstm_n_prev': [16, 32]\n",
    "}\n",
    "\n",
    "# 'Variables' are those hyperparameters with multiple values defined\n",
    "variables = []\n",
    "for key, value in param_sweep_values.items():\n",
    "    if len(value) > 1:\n",
    "        variables.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Documents and DataLoader \n",
    "\n",
    "The training documents are MIDI sequences that the **Midi2Vec** model should learn to imitate and reproduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define note mapper for MIDI file loading\n",
    "note_mapping_config_path = \"../settings/map-to-group.json\"\n",
    "note_mapper = NoteMapper(note_mapping_config_path)\n",
    "\n",
    "# Data loader used to encode MIDI-format training files\n",
    "data_loader = MidiDataLoader(note_mapper)\n",
    "\n",
    "# Define training documents for sequence learning\n",
    "training_docs = [\"../resources/midi/system_shock/cyberspace_clip_16.mid\",\n",
    "                \"../resources/midi/system_shock/end_remix_clip_16.mid\",\n",
    "                \"../resources/midi/system_shock/energy_clip_16.mid\",\n",
    "                \"../resources/midi/system_shock/security_bridge_clip_16.mid\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Callback Function\n",
    "\n",
    "This function is called after every iteration of *SequenceLearner* model training. It controls outputting intermediary results and can trigger early stopping of the evaluation pipeline. Since the space of all hyperparameter combinations to evaluate can be exponentially large, it can make sense to implement reasonable early stopping criteria, i.e. after a given number of single evaluations or a predetermined amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_dfs = []\n",
    "runs_completed = [0]\n",
    "start_time = time.time()\n",
    "hours = 6\n",
    "max_runtime = hours*60*60\n",
    "\n",
    "def callback(params, metrics, abort):\n",
    "    runs_completed[0] += 1\n",
    "    merged = {**params, **metrics}\n",
    "    merged['nn_features'] = ', '.join(merged['nn_features'])\n",
    "    print(\"Completed \" + str(runs_completed[0]) + \" runs.\")\n",
    "    print(merged)\n",
    "    results_dfs.append(pd.DataFrame(merged, index=[0]))\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(str(elapsed_time) + \" seconds elapsed so far.\")\n",
    "    if elapsed_time >= max_runtime:\n",
    "        print(\"Max. runtime reached.\")\n",
    "        abort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brute-force evaluation of 4 hyperparameter combinations.\n",
      "Loaded 100000 documents\n",
      "Loaded 200000 documents\n",
      "Loaded 300000 documents\n",
      "Loaded 400000 documents\n",
      "Loaded 500000 documents\n",
      "Loaded 600000 documents\n",
      "Loaded 700000 documents\n",
      "Loaded 800000 documents\n",
      "Loaded 900000 documents\n",
      "Loaded 1000000 documents\n",
      "Loaded 1100000 documents\n",
      "Loaded 1200000 documents\n",
      "Loaded 1300000 documents\n",
      "Loaded 1400000 documents\n",
      "Loaded 1500000 documents\n",
      "Loaded 1600000 documents\n",
      "Loaded 1700000 documents\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline()\n",
    "pipeline.set_data_loader(data_loader)\n",
    "pipeline.set_training_docs(training_docs)\n",
    "pipeline.set_k_fold_cross_eval(k=4)\n",
    "\n",
    "brute_force_param_sweep = BruteForce(params=param_sweep_values)\n",
    "brute_force_param_sweep.set_callback(callback)\n",
    "pipeline.set_optimizer(brute_force_param_sweep)\n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect and Aggregate Results of Runs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_df = pd.concat(results_dfs)\n",
    "#results_df = pd.read_csv('breakbeats_2018_06_03.csv')\n",
    "# TODO read 'variables' from DF if using saved CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Single Variables (F1 Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Get number of plots to create\n",
    "num_plots = len(variables)       \n",
    "dim = math.ceil(num_plots / 2)\n",
    "\n",
    "# Set up figures for plots\n",
    "fig, axes = plt.subplots(dim, 2, sharex=False, sharey=True)\n",
    "fig.set_size_inches(10, dim*5) # width, height\n",
    "\n",
    "for axis_index, variable in enumerate(variables):\n",
    "    grouped = results_df[[variable,'f1']].groupby([variable], as_index=False).median()\n",
    "    sns.pointplot(x=variable, y='f1', data=grouped, ax=axes.flat[axis_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Variable Interactions (F1 Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "combinations = list(itertools.combinations(variables, r=2))\n",
    "plots_per_row = 2\n",
    "inches_per_plot = 5\n",
    "\n",
    "# Number of plots to render\n",
    "num_plots = len(combinations)            \n",
    "dim = math.ceil(num_plots / plots_per_row)\n",
    "\n",
    "# Set up figures for plots\n",
    "fig, axes = plt.subplots(dim, plots_per_row, sharex=False, sharey=False)\n",
    "fig.set_size_inches(plots_per_row * inches_per_plot, dim * inches_per_plot) # width, height\n",
    "\n",
    "# Plot each pair of variables against each other on a heatmap\n",
    "for axis_index, (heatmap_x, heatmap_y) in enumerate(combinations):\n",
    "    grouped = results_df.groupby([heatmap_y, heatmap_x], as_index=False).median()\n",
    "    pivoted = grouped.pivot(heatmap_y, heatmap_x, \"f1\")\n",
    "    sns.heatmap(pivoted, annot=True, fmt=\"g\", cmap='viridis', ax=axes.flat[axis_index])\n",
    "    axis_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Results of Evaluation Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#results_df.to_csv(\"breakbeats_2018_06_03.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
